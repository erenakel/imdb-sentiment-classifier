{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c92LTnKzQnt6",
        "UVjZLJlZQJAd",
        "ExT6khvFQO57"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis on Movie Reviews (NLP Project)\n",
        "\n",
        "In this project, I will train a model using IMDB reviews and predict whether the review is positive or negative.\n"
      ],
      "metadata": {
        "id": "P-9R2oBp0OUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "c92LTnKzQnt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "t4z-EqoL0J62"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds # For IMDB dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IBDM Dataset\n",
        "\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "\n",
        "# Split data into train and test\n",
        "\n",
        "train_data = imdb['train']\n",
        "test_data = imdb['test']\n",
        "\n",
        "# as_supervised=True\n",
        "# To get the label (0 = negative, 1 = positive) of each review"
      ],
      "metadata": {
        "id": "NXXTK-CN0h7b"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For converting dataset to numpy arrays\n",
        "\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "\n",
        "test_sentences = []\n",
        "test_labels = []"
      ],
      "metadata": {
        "id": "fBLcoKpK0jAf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and test data and add it to the lists.\n",
        "\n",
        "for s, l in train_data:\n",
        "    train_sentences.append(str(s.numpy()))\n",
        "    train_labels.append(l.numpy())\n",
        "\n",
        "for s, l in test_data:\n",
        "    test_sentences.append(str(s.numpy()))\n",
        "    test_labels.append(l.numpy())"
      ],
      "metadata": {
        "id": "jKQYFg2t3GML"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Convert the labels (consisting of 0s and 1s) into a numpy array using np.array().\n",
        "# This way, we can perform faster processing during model training."
      ],
      "metadata": {
        "id": "l6R9BUQU3xFk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print on the screen how many training and test examples we have.\n",
        "\n",
        "print(f\"Dataset Loaded: {len(train_sentences)} training samples, {len(test_sentences)} test samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd288-fg4Kg6",
        "outputId": "61f6a073-376b-41e8-fb57-6d5e46ebfc39"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded: 25000 training samples, 25000 test samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Stage**:\n",
        "\n",
        "- We downloaded the IMDB dataset from TensorFlow.\n",
        "\n",
        "- We converted the comments and labels to numpy arrays.\n",
        "\n",
        "- We checked the training and test sets.\n",
        "\n",
        "\n",
        "**Next Up: Tokenization**"
      ],
      "metadata": {
        "id": "9l5vOkqX4kPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "UVjZLJlZQJAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text #NLP models use this in the background"
      ],
      "metadata": {
        "id": "eWHfbONV7mJ8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextVectorization layer\n",
        "\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=10000,  # Get the 10,000 most frequently used words\n",
        "    output_sequence_length=200  # Fix sentence length to 200 words\n",
        ")"
      ],
      "metadata": {
        "id": "FjYwNY5C9IZx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt tokenizer to training data\n",
        "\n",
        "vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "wMxnzsnKAYAK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "sample_text = [\"The movie was fantastic! I loved it.\"]\n",
        "sample_tokenized = vectorizer(sample_text)\n",
        "\n",
        "print(sample_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg92YJvvAyE5",
        "outputId": "d23e6a24-7775-4d0b-9ec0-07c657a43aff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  2  18  14 771  11 438   9   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]], shape=(1, 200), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all training and testing data\n",
        "\n",
        "train_sequences = vectorizer(np.array(train_sentences))\n",
        "test_sequences = vectorizer(np.array(test_sentences))"
      ],
      "metadata": {
        "id": "2fWVuSnVBLbb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's convert TensorFlow tensors to NumPy arrays\n",
        "\n",
        "train_padded = np.array(train_sequences)\n",
        "test_padded = np.array(test_sequences)"
      ],
      "metadata": {
        "id": "rOi3KCfEBU9b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "\n",
        "print(f\"Train Padded Shape: {train_padded.shape}\")\n",
        "print(f\"Test Padded Shape: {test_padded.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dHPzMvfBax2",
        "outputId": "0fc6ea99-fe02-4229-e191-f31cf0e20b32"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Padded Shape: (25000, 200)\n",
            "Test Padded Shape: (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Stage**:\n",
        "\n",
        "- We converted words to numbers with TextVectorization.\n",
        "- We fixed the length of sentences (Padding).\n",
        "- The model will now see texts as numbers.\n",
        "\n",
        "\n",
        "**Next Up: A LSTM (Long Short-Term Memory) based sentiment analysis model**"
      ],
      "metadata": {
        "id": "Qpwdj0WCBhJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A LSTM (Long Short-Term Memory) Based Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "ExT6khvFQO57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(200,)),\n",
        "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=200),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "VWiMVhJoBg79"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding Layer → Converts words to vectors (Each word will be a 16-dimensional vector).\n",
        "- Bidirectional LSTM → Bidirectional LSTM captures the meaning of sentences better.\n",
        "- Dense Layer (ReLU) → Extra learning layer provides better generalization of the model.\n",
        "- Dense Layer (Sigmoid) → Output layer returns the output as a sentiment score between 0 and 1."
      ],
      "metadata": {
        "id": "wpUIMJBtCp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "PeedGzveDTDM",
        "outputId": "dd07eb65-cc0d-4a76-d54f-9ccc1c1b4c23"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m160,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m41,472\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m41,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,472</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m243,745\u001b[0m (952.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,745</span> (952.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m243,745\u001b[0m (952.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,745</span> (952.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OLSKkxECrnI",
        "outputId": "478ec13c-3193-4197-b140-c2c2a6434bc3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 386ms/step - accuracy: 0.5534 - loss: 0.6746 - val_accuracy: 0.7211 - val_loss: 0.5907\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 423ms/step - accuracy: 0.7931 - loss: 0.4583 - val_accuracy: 0.8197 - val_loss: 0.4104\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 416ms/step - accuracy: 0.8671 - loss: 0.3323 - val_accuracy: 0.8244 - val_loss: 0.4138\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 422ms/step - accuracy: 0.8958 - loss: 0.2694 - val_accuracy: 0.8360 - val_loss: 0.4117\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 416ms/step - accuracy: 0.9186 - loss: 0.2216 - val_accuracy: 0.8340 - val_loss: 0.3956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (80% or more desired)\n",
        "\n",
        "loss, acc = model.evaluate(test_padded, test_labels)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YajqxpOLltJ",
        "outputId": "beeaad73-9c62-4fd5-b4ce-0234023e2295"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 79ms/step - accuracy: 0.8359 - loss: 0.3943\n",
            "Test Accuracy: 0.8340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy: 0.8340**\n",
        "\n",
        "The model achieved 83.4% accuracy, which is a very good starting level result for sentiment analysis.\n",
        "\n",
        "However, the following can be implemented for improvement:\n",
        "\n",
        "\n",
        "- Increase the Embedding Size:\n",
        "```\n",
        "tf.keras.layers.Embedding(input_dim=10000, output_dim=32, input_length=200)\n",
        "```\n",
        "- Make STM Layers Deeper\n",
        "```\n",
        "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))\n",
        "```\n",
        "- Prevent Overfitting with Dropout\n",
        "```\n",
        "tf.keras.layers.Dropout(0.3)\n",
        "```\n",
        "**But for now I want to leave it like this and move on.**"
      ],
      "metadata": {
        "id": "9ZlXdf-LL-xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this step:**\n",
        "\n",
        "- We created a sentiment analysis model based on LSTM.\n",
        "\n",
        "- We trained the model with train_padded data.\n",
        "\n",
        "- We calculated the accuracy of the model on the test data.\n",
        "\n",
        "\n",
        "**Next Up: Testing the Model with Real Comments**"
      ],
      "metadata": {
        "id": "g1GbzgM2MGHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model with Real Comments"
      ],
      "metadata": {
        "id": "Xk28uq6aQaBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with a sample user text\n",
        "\n",
        "sample_text = [\"The movie was absolutely amazing, I loved it!\"]\n",
        "\n",
        "# Vectorizing\n",
        "\n",
        "sample_sequence = vectorizer(sample_text)\n",
        "sample_padded = tf.cast(sample_sequence, tf.int32) # data type must be int32"
      ],
      "metadata": {
        "id": "LJgm4AbrMS9B"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model prediction\n",
        "\n",
        "prediction = model.predict(sample_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ISzVWdOm1E",
        "outputId": "59ed6f16-ccea-4253-d0de-6c2e6dfd4146"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the result\n",
        "\n",
        "print(f\"Sentiment Score: {prediction[0][0]:.4f}\")\n",
        "\n",
        "if prediction[0][0] > 0.5:\n",
        "  print(\"Prediction: Positive Comment 🙂\")\n",
        "else:\n",
        "  print(\"Prediction: Negative Comment 😔\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdSSkBTwOuLm",
        "outputId": "2cbde050-008b-483a-dd41-1346c8387028"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Score: 0.9544\n",
            "Prediction: Positive Comment 🙂\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More examples\n",
        "\n",
        "test_sentences = [\n",
        "    \"I really enjoyed this movie, it was fantastic!\",\n",
        "    \"The plot was very boring and the acting was terrible.\",\n",
        "    \"One of the best movies I have ever seen.\",\n",
        "    \"I would never watch this movie again, complete waste of time!\",\n",
        "    \"Not bad, but could have been better.\",\n",
        "    \"An absolute masterpiece! Highly recommend it.\",\n",
        "    \"I fell asleep while watching, not entertaining at all.\"\n",
        "]\n",
        "\n",
        "# Tokenize\n",
        "\n",
        "test_sequences = vectorizer(test_sentences)\n",
        "test_padded = tf.cast(test_sequences, tf.int32)\n",
        "\n",
        "# Prediction\n",
        "\n",
        "predictions = model.predict(test_padded)\n",
        "\n",
        "# Printing\n",
        "\n",
        "for i, text in enumerate(test_sentences):\n",
        "    score = predictions[i][0]\n",
        "    sentiment = \"Positive 🙂\" if score > 0.5 else \"Negative 😔\"\n",
        "    print(f\"Comment: {text}\\nPrediction: {sentiment} (Score: {score:.4f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6HfOZdnPDf9",
        "outputId": "405a9730-5005-4ba6-954f-7bf46f648a81"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 807ms/step\n",
            "Comment: I really enjoyed this movie, it was fantastic!\n",
            "Prediction: Positive 🙂 (Score: 0.9531)\n",
            "\n",
            "Comment: The plot was very boring and the acting was terrible.\n",
            "Prediction: Negative 😔 (Score: 0.1333)\n",
            "\n",
            "Comment: One of the best movies I have ever seen.\n",
            "Prediction: Positive 🙂 (Score: 0.9287)\n",
            "\n",
            "Comment: I would never watch this movie again, complete waste of time!\n",
            "Prediction: Negative 😔 (Score: 0.2672)\n",
            "\n",
            "Comment: Not bad, but could have been better.\n",
            "Prediction: Positive 🙂 (Score: 0.6611)\n",
            "\n",
            "Comment: An absolute masterpiece! Highly recommend it.\n",
            "Prediction: Positive 🙂 (Score: 0.9638)\n",
            "\n",
            "Comment: I fell asleep while watching, not entertaining at all.\n",
            "Prediction: Negative 😔 (Score: 0.4440)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Stage**:\n",
        "\n",
        "- We tested the model with real user comments.\n",
        "- We observed the model working on a single comment and multiple comments.\n",
        "- We checked the accuracy of the predictions and evaluated the model.\n",
        "\n",
        "Next Up: Deploying the Model (Publishing as API)"
      ],
      "metadata": {
        "id": "KopsvnJfPRq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying the Model (Publishing as API)"
      ],
      "metadata": {
        "id": "I6kxx494Qda6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Modeli Drive'a kaydet\n",
        "model.save('/content/drive/MyDrive/sentiment_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2-_6cgWPcg-",
        "outputId": "f7e239c9-6364-48a2-bae9-dd792c146e5e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "\n",
        "!pip install flask flask-ngrok pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x1h31MgES296",
        "outputId": "87812191-d39a-4529-ba93-d96f5ca603ec"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.1.31)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ngrok token\n",
        "\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = input(\"Enter your ngrok auth token: \")"
      ],
      "metadata": {
        "id": "fpWjqg61Wm8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken $NGROK_AUTH_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCRxWRRsXRy_",
        "outputId": "4d3ed493-1f78-4e0e-b69c-67838bd4e601"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/sentiment_model.keras\", compile=False)\n",
        "\n",
        "print(\"Model loaded successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5gqUFPqTc8O",
        "outputId": "8b68b56c-1109-4fb8-b806-54a31504bbd2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask API\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ngrok connection\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# API Endpoint: /predict\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    data = request.get_json()  # Retrieve the JSON data sent by the user\n",
        "    text = data[\"text\"]  # Extract the text from the JSON request\n",
        "\n",
        "    # Tokenize the text\n",
        "\n",
        "    sequence = vectorizer([text])\n",
        "    padded_sequence = tf.cast(sequence, tf.int32)\n",
        "\n",
        "    # Get the model's prediction\n",
        "\n",
        "    prediction = model.predict(padded_sequence)[0][0]\n",
        "\n",
        "    # Determine the sentiment\n",
        "\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "\n",
        "    # Return the response as JSON\n",
        "\n",
        "    return jsonify({\"text\": text, \"sentiment\": sentiment, \"score\": float(prediction)})\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "0NPtbazrUQZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL\n",
        "\n",
        "url = \"https://aaef-35-189-190-47.ngrok-free.app/predict\"\n",
        "\n",
        "# Test\n",
        "\n",
        "data = {\"text\": \"This movie was absolutely amazing! I loved it.\"}\n",
        "\n",
        "# POST\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "# Response\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "ry20e893ZC4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the Stage:\n",
        "\n",
        "- We deployed the trained sentiment analysis model as a Flask API.\n",
        "\n",
        "- We used ngrok to make the API accessible from external sources, allowing real-time sentiment predictions.\n",
        "\n",
        "- The API successfully receives text input, processes it, and returns a classification result."
      ],
      "metadata": {
        "id": "xZWPoYJ1eMyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this project, we built a sentiment analysis model using a bi-directional LSTM network with TensorFlow. The model was trained on a dataset of text reviews, tokenized using the TextVectorization layer, and evaluated for accuracy. After achieving a satisfactory performance, we deployed the model as a Flask API, making it accessible for real-time predictions.\n",
        "\n",
        "To integrate the model with an external interface, we used ngrok, allowing API access from anywhere. The API successfully receives text input, processes it, and returns a sentiment prediction.\n",
        "\n",
        "This project demonstrates the complete pipeline of a machine learning model, from training and evaluation to deployment as a web service. Future improvements could include fine-tuning with Transformer models, deploying on a cloud platform, and integrating with a web-based user interface for a more interactive experience."
      ],
      "metadata": {
        "id": "ewfwyFR-eQjf"
      }
    }
  ]
}